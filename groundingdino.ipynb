{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Packages\n",
    "\n",
    "Packages contain pre-defined functions. They are needed for GD to run properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch \n",
    "%pip install torchvision\n",
    "%pip install requests\n",
    "%pip install supervision\n",
    "%pip install transformers\n",
    "%pip install addict\n",
    "%pip install yapf\n",
    "%pip install timm\n",
    "%pip install numpy\n",
    "%pip install opencv-python\n",
    "%pip install supervision\n",
    "%pip install cython\n",
    "%pip install pycocotools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cloning Grounding Dino\n",
    "GD is currently published online in a *repo*, a platform publicly hosting code. We will clone it into the machine so we can use it freely. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we import the `os` library, which allows us to interact with the operating system. We then use `os.getcwd()` to get the current working directory, which will be stored in a variable named `projectdir`. Finally, we print the value of `projectdir` to verify the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "projectdir = os.getcwd()\n",
    "projectdir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We import the `torch` library for deep learning and the `requests` library for downloading files from the internet. We also clone the GroundingDINO repository from GitHub, which contains the code we'll be using in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "# Clone the repository\n",
    "os.system(\"git clone https://github.com/IDEA-Research/GroundingDINO.git\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Download Weight File\n",
    "When a machine learning model is trained, the information it has learnt is saved as a **model**. \n",
    "Here, we will be downloading an already-existing weight file so that our model knows how to identify objects from the get go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we create a directory called `weights` to store the weight file for our model. We then change our working directory to `weights` and download the weight file from a specified URL using the `requests` library. If the download is successful, we save the file in the `weights` directory and print a confirmation message. Otherwise, we print an error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for weights\n",
    "weights_dir = os.path.join(projectdir,\"weights\")\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Change directory to the weights directory\n",
    "os.chdir(weights_dir)\n",
    "# Download the weight file\n",
    "weight_url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
    "weight_filename = os.path.basename(weight_url)\n",
    "weight_filepath = os.path.join(weights_dir, weight_filename)\n",
    "\n",
    "response = requests.get(weight_url)\n",
    "if response.status_code == 200:\n",
    "    with open(weight_filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Weight file downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download weight file. Status code: {response.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download data image sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a directory called `data` to store images. We then change our working directory to `data` and download a set of images from specified URLs using the `requests` library. Each image is saved in the `data` directory with its respective filename. A confirmation message is printed for each successful download, and an error message is printed if a download fails.\n",
    "Note that these images are only being downloaded for demo purposes in the workshop -- you can also add your own images later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for data\n",
    "data_dir = os.path.join(projectdir,\"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Change directory to the data directory\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# URLs of the images to download\n",
    "image_urls = {\n",
    "    \"compass.jpg\": \"https://unsplash.com/photos/xu2WYJek5AI/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MTV8fGNvbXBhc3N8ZW58MHx8fHwxNjg5MTc2NzMyfDA&force=true&w=960\",\n",
    "    \"air.jpg\": \"https://unsplash.com/photos/AlA8S9tALAs/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MTR8fHBhcmFjaHV0ZXxlbnwwfHx8fDE2ODkwOTU1MTJ8MA&force=true&w=960\",\n",
    "    \"ocean.jpg\": \"https://unsplash.com/photos/1PWhYZ_erME/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjg5MDA2MTk5fA&force=true&w=960\",\n",
    "    \"snow.jpg\": \"https://unsplash.com/photos/MB1FuEh0AzU/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8NHx8c25vd2JvYXJkZXJzfGVufDB8MHx8fDE2ODkwMTk0NTB8MA&force=true&w=960\",\n",
    "    \"hardware.jpg\": \"https://unsplash.com/photos/lllK4-63KTw/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8Ym9sdCUyMGFuZCUyMHdhc2hlcnxlbnwwfHx8fDE2ODkxNzg1NTN8MA&force=true&w=960\"\n",
    "}\n",
    "\n",
    "# Download each image\n",
    "for filename, url in image_urls.items():\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(data_dir, filename), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"{filename} downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}. Status code: {response.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model\n",
    "In this section, we import the necessary modules and define paths for the GroundingDINO model configuration and weights. We then use the `load_model` function from the `groundingdino.util.inference` module to load the model with the specified configuration and weights. This prepares our model to be used right away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(projectdir)\n",
    "%cd GroundingDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model\n",
    "\n",
    "# Define paths\n",
    "groundingdino_dir = os.path.join(projectdir, \"GroundingDINO\")\n",
    "model_config_path = os.path.join(groundingdino_dir, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "weights_path = os.path.join(projectdir, \"weights/groundingdino_swint_ogc.pth\")\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_config_path, weights_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Play with GroundingDINOðŸ¦–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we perform object detection using the loaded GroundingDINO model. We define the image name, path, and text prompt for the detection, along with thresholds for boxes and text. We load the image using the `load_image` function and then use the `predict` function to perform object detection based on the specified text prompt. The detected objects are then annotated on the image using the `annotate` function. Finally, we display the annotated image using the `plot_image` function from the `supervision` module. This demonstrates the ability of our model to detect and highlight objects in an image based on textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from groundingdino.util.inference import load_image, predict, annotate\n",
    "\n",
    "# Define constants and paths\n",
    "IMAGE_NAME = \"compass.jpg\"\n",
    "IMAGE_PATH = os.path.join(projectdir, \"data\", IMAGE_NAME)\n",
    "TEXT_PROMPT = \"compass\"\n",
    "BOX_THRESHOLD = 0.70\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cpu\"  # Specify \"cpu\" as the device\n",
    "\n",
    "# Load image\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "print(image.shape)\n",
    "\n",
    "# Perform object detection\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE  # Pass \"cpu\" as the device\n",
    ")\n",
    "\n",
    "# Annotate the image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "sv.plot_image(annotated_frame, (16, 16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we apply the GroundingDINO model for object detection on a different image and text prompt. We define the image name as \"hardware.jpg\" and the text prompt as \"spanner\". \n",
    "\n",
    "The same process is followed as before: we load the image, perform object detection using the `predict` function with the specified text prompt and thresholds, and then annotate the detected objects on the image. Finally, we display the annotated image to visualize the results of our object detection task. This showcases the versatility of our model in detecting various objects based on textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and paths\n",
    "IMAGE_NAME = \"hardware.jpg\"\n",
    "IMAGE_PATH = os.path.join(projectdir, \"data\", IMAGE_NAME)\n",
    "TEXT_PROMPT = \"spanner\"\n",
    "BOX_THRESHOLD = 0.70\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cpu\"  # Specify \"cpu\" as the device\n",
    "\n",
    "# Load image\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "# Perform object detection\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE  # Pass \"cpu\" as the device\n",
    ")\n",
    "\n",
    "# Annotate the image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "sv.plot_image(annotated_frame, (16, 16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD + OPENCV/ Open Webcam and Snap frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we expand the application of the GroundingDINO model to perform object detection on frames captured from a webcam. We define constants and paths for capturing and storing frames, along with the text prompt \"face\" for object detection. The process involves capturing frames at specified intervals, performing object detection on each frame using the `predict` function, annotating the detected objects, and finally displaying the annotated frames. This demonstrates how our model can be applied to real-time object detection in video streams or live camera feeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import load_image, predict, annotate\n",
    "\n",
    "# Constants and paths\n",
    "IMAGE_FOLDER = \"snap\"\n",
    "TEXT_PROMPT = \"face\"\n",
    "BOX_THRESHOLD = 0.50\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cpu\"  # Specify \"cpu\" as the device\n",
    "SNAP_INTERVAL = 2   # Interval to capture frames (in seconds)\n",
    "NUM_SNAPS = 5     # Total number of frames to capture\n",
    "\n",
    "# Change directory to the data directory\n",
    "os.chdir(projectdir)\n",
    "\n",
    "# Check if the image folder exists and delete its contents if it does\n",
    "if os.path.exists(IMAGE_FOLDER):\n",
    "    for filename in os.listdir(IMAGE_FOLDER):\n",
    "        file_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "else:\n",
    "    os.makedirs(IMAGE_FOLDER)\n",
    "\n",
    "# Function to capture frames from webcam and save them to the folder\n",
    "def capture_frames(folder, interval, num_snaps):\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "    \n",
    "    time.sleep(2)  # Delay start by 2 seconds\n",
    "    \n",
    "    frame_count = 0\n",
    "    while frame_count < num_snaps:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow('snap', frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Save frame every interval seconds\n",
    "        image_name = f\"snap_{frame_count}.jpg\"\n",
    "        cv2.imwrite(os.path.join(folder, image_name), frame)\n",
    "\n",
    "        time.sleep(interval)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Function to annotate images in the folder\n",
    "def annotate_images(folder):\n",
    "    annotated_images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder, filename)\n",
    "            image_source, image = load_image(image_path)\n",
    "\n",
    "            # Perform object detection\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=model,\n",
    "                image=image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_THRESHOLD,\n",
    "                text_threshold=TEXT_THRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            # Annotate the image\n",
    "            annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "            annotated_images.append((image_path, annotated_frame))\n",
    "\n",
    "    return annotated_images\n",
    "\n",
    "# Capture frames from webcam and save them\n",
    "capture_frames(IMAGE_FOLDER,SNAP_INTERVAL, NUM_SNAPS)\n",
    "\n",
    "# Annotate images in the folder and replace them\n",
    "annotated_images = annotate_images(IMAGE_FOLDER)\n",
    "for image_path, annotated_frame in annotated_images:\n",
    "    cv2.imwrite(image_path, annotated_frame)\n",
    "\n",
    "# Display annotated images\n",
    "for image_path, annotated_frame in annotated_images:\n",
    "    sv.plot_image(annotated_frame, (16, 16), f\"Annotated Image: {image_path}\")\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, time to use your own images!\n",
    "Here, as long as you only edit the values of the variables below, you can use your own images for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the name of the files here\n",
    "image_names = [\"example1.jpg\",]\n",
    "image_prompts = [\"prompt for example 1\",] #add them in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "BOX_THRESHOLD = 0.70\n",
    "TEXT_THRESHOLD = 0.25\n",
    "image_sources, processed_images = [], []\n",
    "for image, prompt in image_names, image_prompts:\n",
    "    path = os.path.join(projectdir, \"data\", image)\n",
    "    s,i = load_image(path)\n",
    "    image_sources.append(s)\n",
    "    processed_images.append(i)\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=prompt,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE  # Pass \"cpu\" as the device\n",
    ")\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "sv.plot_image(annotated_frame, (16, 16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
