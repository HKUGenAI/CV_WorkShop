{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Packages\n",
    "\n",
    "Packages contain pre-defined functions. They are needed for GD to run properly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cloning Grounding Dino\n",
    "GD is currently published online in a *repo*, a platform publicly hosting code. We will clone it into the machine so we can use it freely. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we import the `os` library, which allows us to interact with the operating system. We then use `os.getcwd()` to get the current working directory, which will be stored in a variable named `projectdir`. Finally, we print the value of `projectdir` to verify the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "projectdir = os.getcwd()\n",
    "projectdir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We import the `torch` library for deep learning and the `requests` library for downloading files from the internet. We also clone the GroundingDINO repository from GitHub, which contains the code we'll be using in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "# Clone the repository\n",
    "os.system(\"git clone https://github.com/IDEA-Research/GroundingDINO.git\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Download Weight File\n",
    "When a machine learning model is trained, the information it has learnt is saved as a **model**. \n",
    "Here, we will be downloading an already-existing weight file so that our model knows how to identify objects from the get go!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we create a directory called `weights` to store the weight file for our model. We then change our working directory to `weights` and download the weight file from a specified URL using the `requests` library. If the download is successful, we save the file in the `weights` directory and print a confirmation message. Otherwise, we print an error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for weights\n",
    "weights_dir = os.path.join(projectdir,\"weights\")\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Change directory to the weights directory\n",
    "os.chdir(weights_dir)\n",
    "# Download the weight file\n",
    "weight_url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
    "weight_filename = os.path.basename(weight_url)\n",
    "weight_filepath = os.path.join(weights_dir, weight_filename)\n",
    "\n",
    "response = requests.get(weight_url)\n",
    "if response.status_code == 200:\n",
    "    with open(weight_filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Weight file downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download weight file. Status code: {response.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model\n",
    "In this section, we import the necessary modules and define paths for the GroundingDINO model configuration and weights. We then use the `load_model` function from the `groundingdino.util.inference` module to load the model with the specified configuration and weights. This prepares our model to be used right away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(projectdir)\n",
    "%cd GroundingDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model\n",
    "\n",
    "# Define paths\n",
    "groundingdino_dir = os.path.join(projectdir, \"GroundingDINO\")\n",
    "model_config_path = os.path.join(groundingdino_dir, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "weights_path = os.path.join(projectdir, \"weights/groundingdino_swint_ogc.pth\")\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_config_path, weights_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Play with GroundingDINOðŸ¦–"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we perform object detection using the loaded GroundingDINO model. We define the image name, path, and text prompt for the detection, along with thresholds for boxes and text. We load the image using the `load_image` function and then use the `predict` function to perform object detection based on the specified text prompt. The detected objects are then annotated on the image using the `annotate` function. Finally, we display the annotated image using the `plot_image` function from the `supervision` module. This demonstrates the ability of our model to detect and highlight objects in an image based on textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from groundingdino.util.inference import load_image, predict, annotate\n",
    "\n",
    "# Define constants and paths\n",
    "IMAGE_NAME = \"compass.jpg\"\n",
    "IMAGE_PATH = os.path.join(projectdir, \"datasets_GD\", IMAGE_NAME)\n",
    "TEXT_PROMPT = \"compass\"\n",
    "BOX_THRESHOLD = 0.70\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cpu\"  # Specify \"cpu\" as the device\n",
    "\n",
    "# Load image\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "print(image.shape)\n",
    "\n",
    "# Perform object detection\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE  # Pass \"cpu\" as the device\n",
    ")\n",
    "\n",
    "# Annotate the image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "sv.plot_image(annotated_frame, (16, 16))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we apply the GroundingDINO model for object detection on a different image and text prompt. We define the image name as \"hardware.jpg\" and the text prompt as \"spanner\". \n",
    "\n",
    "The same process is followed as before: we load the image, perform object detection using the `predict` function with the specified text prompt and thresholds, and then annotate the detected objects on the image. Finally, we display the annotated image to visualize the results of our object detection task. This showcases the versatility of our model in detecting various objects based on textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and paths\n",
    "IMAGE_NAME = \"hardware.jpg\"\n",
    "IMAGE_PATH = os.path.join(projectdir, \"datasets_GD\", IMAGE_NAME)\n",
    "TEXT_PROMPT = \"spanner\"\n",
    "BOX_THRESHOLD = 0.70\n",
    "TEXT_THRESHOLD = 0.25\n",
    "DEVICE = \"cpu\"  # Specify \"cpu\" as the device\n",
    "\n",
    "# Load image\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "# Perform object detection\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE  # Pass \"cpu\" as the device\n",
    ")\n",
    "\n",
    "# Annotate the image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "sv.plot_image(annotated_frame, (16, 16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, time to use your own images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
